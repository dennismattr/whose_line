{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# EDA imports\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import iqr\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk as nl\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.parts_of_speech import PROPN\n",
    "\n",
    "# Charts\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file):\n",
    "    '''\n",
    "    Opens file by name, reads text into a string and \n",
    "    then closes that file. Returns a string of full text.\n",
    "    \n",
    "    File must be in .txt format.\n",
    "    '''\n",
    "    \n",
    "    path = './{}/{}'.format(folder, file) # Folder set in imports\n",
    "    fragments = ['\\n', '\\ufeff']\n",
    "    \n",
    "    s = open(path, 'r').read().strip()\n",
    "    \n",
    "    for item in fragments:\n",
    "        s = s.replace(item, ' ')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_author_dict(dictionary, file):\n",
    "    '''\n",
    "    Update the dictionary with passed file. Fills the author, \n",
    "    title, and text of dictionary.\n",
    "    '''\n",
    "\n",
    "    # Split the title from the filepath at the hyphen\n",
    "    author = file.split('-')[0]\n",
    "    # Split the author name from the filepath after the hyphen\n",
    "    title = file.split('-')[1]\n",
    "    # Remove the .txt from the author name\n",
    "    title = title.split('.')[0]\n",
    "    # Call the load text function on the given file\n",
    "    text = load_text(file)\n",
    "    \n",
    "    # Enter text from file with respective author and title\n",
    "    if author in dictionary:\n",
    "        dictionary[author].update({title:text})\n",
    "    else:\n",
    "        dictionary.update({author: {title: text}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_proper_nouns(s):\n",
    "    '''\n",
    "    Finds proper nouns in row, masks the nouns with nnnn\n",
    "    then returns the rows\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    s = nlp(s)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in s:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            tokens.append('nnnn')\n",
    "        else:\n",
    "            tokens.append(token.text)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    '''\n",
    "    Convert special quotes and apostrophes to basic quotes and apostrophes,\n",
    "    select only text and symbols specified by the regex below, and finally\n",
    "    replace any quotes with mask strings of 'qqqq'\n",
    "    '''\n",
    "    s = s.replace('”', '\"')\n",
    "    s = s.replace('“', '\"')\n",
    "    s = s.replace('’', \"'\")\n",
    "#     s = re.sub(\"\\\".*?\\\"\", ' ', s)\n",
    "#     s = re.sub(\"[^A-z _.,;:\\\\p{Pd}!'\\\"]\", ' ', s) #\\\\p{Pd} should pick up all types of dash chars\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_only(row):\n",
    "    '''\n",
    "    Uses regex to keep only characters in given text\n",
    "    '''\n",
    "    return re.sub(\"[^A-z]\", ' ', row).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lengths(author):\n",
    "    '''\n",
    "    Apply function to grab the length of given sentence per author\n",
    "    '''\n",
    "    lengths = []\n",
    "    \n",
    "    [lengths.append(len(row.split())) for row in data[data['author'] == author]['sentence']]\n",
    "    \n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kde_output(X_plot, series, bandwidth):\n",
    "    '''\n",
    "    Creates plottable y-axis from KDE\n",
    "    '''\n",
    "    X = np.array(series).reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(X)\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    \n",
    "    return np.exp(log_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(word_list, author, target):\n",
    "    '''\n",
    "    Counts given target word by author and returns that count\n",
    "    '''\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word[1] == author:\n",
    "            if word[0] == target:\n",
    "                word_count += 1\n",
    "            \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in Data\n",
    "Read .txt files into strings for each novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict = {}\n",
    "\n",
    "path = f'./{folder}'\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "for file in files[1:]:\n",
    "    update_author_dict(author_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK calls the PUNKT unsupervised model for splitting a string into sentences. This output is saved as *_sentences that will be labeled and later, further tokenized.\n",
    "\n",
    "This process is done on every title for every author in the author_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in author_dict:\n",
    "    for title in author_dict[author]:\n",
    "        author_dict[author][title] = clean_text(author_dict[author][title])\n",
    "        author_dict[author][title] = mask_proper_nouns(author_dict[author][title])\n",
    "        author_dict[author][title] = nl.sent_tokenize(author_dict[author][title])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame, data, that will hold set of random sentences for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vonnegut     6348\n",
       "hemingway    6348\n",
       "nabokov      6348\n",
       "woolf        6348\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This parameter controls the minimum character count to include a sentence\n",
    "min_sentence_length = 60\n",
    "# Instantiate DataFrame to hold set of sentences to train model on\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Loop through all authors and their works, select random sentences\n",
    "# Add these random sentences to data\n",
    "for author in author_dict: \n",
    "    for title in author_dict[author]:\n",
    "        # Create temp DataFrame work with before adding to data\n",
    "        temp = pd.DataFrame(author_dict[author][title], columns=['sentence'])\n",
    "        temp['author'] = author\n",
    "        temp['title'] = title\n",
    "        \n",
    "        # Only keep sentences equal to or longer than the specified length\n",
    "        temp = temp[temp['sentence'].str.len() > min_sentence_length].copy()\n",
    "        \n",
    "        # Split sentences over specified length\n",
    "        \n",
    "        \n",
    "        # Concatenate the temp df onto the primary DataFrame\n",
    "        data = pd.concat([data, temp], axis=0, ignore_index=True)\n",
    "        \n",
    "# Force all sentences to be lowercase\n",
    "data['sentence'] = data['sentence'].map(char_only)\n",
    "\n",
    "# Find the number of rows in the limiting class\n",
    "row_limit = min(data['author'].value_counts())\n",
    "\n",
    "# Balance classes\n",
    "drop_indices = []\n",
    "\n",
    "for author in data['author'].unique():\n",
    "    # Count of rows in current author class\n",
    "    class_count = len(data[data['author'] == author].values)\n",
    "    # Calculate rows to drop as the difference between current class and limiting class\n",
    "    drop_count = class_count - row_limit\n",
    "    drop_indices.extend(np.random.choice(data[data['author'] == author].index, \n",
    "                                         size=drop_count, \n",
    "                                         replace=False))\n",
    "\n",
    "# Drop the randomly chosen indices to balance classes\n",
    "data.drop(data.index[drop_indices], inplace=True)\n",
    "\n",
    "# Sanity check value vount\n",
    "data['author'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in the late summer of that year we lived in a ...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the bed of the river there were pebbles and...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>troops went by the house and down the road and...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the trunks of the trees too were dusty and the...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the plain was rich with crops   there were man...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     author  \\\n",
       "0  in the late summer of that year we lived in a ...  hemingway   \n",
       "1  in the bed of the river there were pebbles and...  hemingway   \n",
       "2  troops went by the house and down the road and...  hemingway   \n",
       "3  the trunks of the trees too were dusty and the...  hemingway   \n",
       "4  the plain was rich with crops   there were man...  hemingway   \n",
       "\n",
       "                title  \n",
       "0  a farewell to arms  \n",
       "1  a farewell to arms  \n",
       "2  a farewell to arms  \n",
       "3  a farewell to arms  \n",
       "4  a farewell to arms  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25392"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word list for use in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words are stored with their authors in a tuple\n",
    "word_list = []\n",
    "for index, row in enumerate(data['sentence']):\n",
    "    \n",
    "    for word in row.split():\n",
    "        entry = (word, data['author'][index])\n",
    "    \n",
    "        word_list.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create author list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of authors to be used in EDA loops\n",
    "authors = ['hemingway', 'nabokov', 'vonnegut', 'woolf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmn_sentence_lengths = get_sentence_lengths('hemingway')\n",
    "nbk_sentence_lengths = get_sentence_lengths('nabokov')\n",
    "vng_sentence_lengths = get_sentence_lengths('vonnegut')\n",
    "wlf_sentence_lengths = get_sentence_lengths('woolf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max needed was 256 from virginia woolf\n",
    "X_sent_plot = np.array(np.linspace(0, 256, 1000)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create KDE for sentence length distribution\n",
    "y_sent_plot = get_kde_output(X_sent_plot, hmn_sentence_lengths, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['white', 'black', 'red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "\n",
    "for author in authors:\n",
    "    for color in color_list:\n",
    "        print(f'{author} {color} {word_count(word_list, author, color)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmn_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'hemingway':\n",
    "        hmn_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbk_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'nabokov':\n",
    "        nbk_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vng_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'vonnegut':\n",
    "        vng_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlf_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'woolf':\n",
    "        wlf_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded by 0 to the largest word at 18 chars\n",
    "X_word_plot = np.array(np.linspace(0, 18, 1000)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create KDE for word length distribution\n",
    "y_word_plot = get_kde_output(X_word_plot, hmn_word_lengths, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting counts of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in authors:\n",
    "    nouns = []\n",
    "    \n",
    "    for row in data[data['author'] == author]['sentence']:\n",
    "        for word in row.split():\n",
    "            if word == 'nnnn':\n",
    "                nouns.append(word)\n",
    "                \n",
    "    print(f'{author} total nouns: {len(nouns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get totals for scaling charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in authors:\n",
    "    total_sents = []\n",
    "\n",
    "    for row in data[data['author'] == author]['sentence']:\n",
    "        total_sents.append(row)\n",
    "\n",
    "    print(f'{author} total sentences: {len(total_sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in authors:\n",
    "    total_words = []\n",
    "\n",
    "    for row in data[data['author'] == author]['sentence']:\n",
    "        for word in row.split():\n",
    "            total_words.append(word)\n",
    "\n",
    "    print(f'{author} total words: {len(total_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "Save the DataFrame to a .csv file to be read by the modeling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set destination folder and filename\n",
    "# folder = 'DATA/CHECK'\n",
    "# filename = 'UNIQUE_NAME'\n",
    "\n",
    "# # Writout .csv\n",
    "# data.to_csv(f'./{folder}/{filename}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
