{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import iqr\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk as nl\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.parts_of_speech import PROPN\n",
    "\n",
    "# Charts\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file):\n",
    "    '''\n",
    "    Opens file by name, reads text into a string and \n",
    "    then closes that file. Returns a string of full text.\n",
    "    \n",
    "    File must be in .txt format.\n",
    "    '''\n",
    "    \n",
    "    path = './{}/{}'.format(folder, file) # Folder set in imports\n",
    "    fragments = ['\\n', '\\ufeff']\n",
    "    \n",
    "    s = open(path, 'r').read().strip()\n",
    "    \n",
    "    for item in fragments:\n",
    "        s = s.replace(item, ' ')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_author_dict(dictionary, file):\n",
    "    '''\n",
    "    Update the dictionary with passed file. Fills the author, \n",
    "    title, and text of dictionary.\n",
    "    '''\n",
    "\n",
    "    # Split the title from the filepath at the hyphen\n",
    "    author = file.split('-')[0]\n",
    "    # Split the author name from the filepath after the hyphen\n",
    "    title = file.split('-')[1]\n",
    "    # Remove the .txt from the author name\n",
    "    title = title.split('.')[0]\n",
    "    # Call the load text function on the given file\n",
    "    text = load_text(file)\n",
    "    \n",
    "    # Enter text from file with respective author and title\n",
    "    if author in dictionary:\n",
    "        dictionary[author].update({title:text})\n",
    "    else:\n",
    "        dictionary.update({author: {title: text}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_proper_nouns(s):\n",
    "    '''\n",
    "    Finds proper nouns in row, masks the nouns with nnnn\n",
    "    then returns the rows\n",
    "    '''\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    s = nlp(s)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in s:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            tokens.append('nnnn')\n",
    "        else:\n",
    "            tokens.append(token.text)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    '''\n",
    "    Convert special quotes and apostrophes to basic quotes and apostrophes,\n",
    "    select only text and symbols specified by the regex below, and finally\n",
    "    replace any quotes with mask strings of 'qqqq'\n",
    "    '''\n",
    "    s = s.replace('”', '\"')\n",
    "    s = s.replace('“', '\"')\n",
    "    s = s.replace('’', \"'\")\n",
    "#     s = re.sub(\"\\\".*?\\\"\", ' ', s)\n",
    "#     s = re.sub(\"[^A-z _.,;:\\\\p{Pd}!'\\\"]\", ' ', s) #\\\\p{Pd} should pick up all types of dash chars\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_only(row):\n",
    "    '''\n",
    "    Uses regex to keep only characters in given text\n",
    "    '''\n",
    "    return re.sub(\"[^A-z]\", ' ', row).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lengths(author):\n",
    "    lengths = []\n",
    "    \n",
    "    [lengths.append(len(row.split())) for row in data[data['author'] == author]['sentence']]\n",
    "    \n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kde_output(X_plot, series, bandwidth):\n",
    "    '''\n",
    "    Creates plottable y-axis from KDE\n",
    "    '''\n",
    "    X = np.array(series).reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(X)\n",
    "    log_dens = kde.score_samples(X_plot)\n",
    "    \n",
    "    return np.exp(log_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(word_list, author, target):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word[1] == author:\n",
    "            if word[0] == target:\n",
    "                word_count += 1\n",
    "            \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_sequence(df, author):\n",
    "    target = 'nnnn'\n",
    "    \n",
    "    sequence = []\n",
    "    \n",
    "    for sentence in df[df['author']==author]['sentence']:\n",
    "        for word in sentence.split():\n",
    "            if word == target:\n",
    "                sequence.append(1)\n",
    "            else:\n",
    "                sequence.append(0)\n",
    "            \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in Data\n",
    "Read .txt files into strings for each novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict = {}\n",
    "\n",
    "path = f'./{folder}'\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "for file in files[1:]:\n",
    "    update_author_dict(author_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK calls the PUNKT unsupervised model for splitting a string into sentences. This output is saved as *_sentences that will be labeled and later, further tokenized.\n",
    "\n",
    "This process is done on every title for every author in the author_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in author_dict:\n",
    "    for title in author_dict[author]:\n",
    "        author_dict[author][title] = clean_text(author_dict[author][title])\n",
    "        author_dict[author][title] = mask_proper_nouns(author_dict[author][title])\n",
    "        author_dict[author][title] = nl.sent_tokenize(author_dict[author][title])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame, data, that will hold set of random sentences for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woolf        6348\n",
       "nabokov      6348\n",
       "vonnegut     6348\n",
       "hemingway    6348\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This parameter controls the minimum character count to include a sentence\n",
    "min_sentence_length = 60\n",
    "# Instantiate DataFrame to hold set of sentences to train model on\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Loop through all authors and their works, select random sentences\n",
    "# Add these random sentences to data\n",
    "for author in author_dict: \n",
    "    for title in author_dict[author]:\n",
    "        # Create temp DataFrame work with before adding to data\n",
    "        temp = pd.DataFrame(author_dict[author][title], columns=['sentence'])\n",
    "        temp['author'] = author\n",
    "        temp['title'] = title\n",
    "        \n",
    "        # Only keep sentences equal to or longer than the specified length\n",
    "        temp = temp[temp['sentence'].str.len() > min_sentence_length].copy()\n",
    "        \n",
    "        # Split sentences over specified length\n",
    "        \n",
    "        \n",
    "        # Concatenate the temp df onto the primary DataFrame\n",
    "        data = pd.concat([data, temp], axis=0, ignore_index=True)\n",
    "        \n",
    "# Force all sentences to be lowercase\n",
    "data['sentence'] = data['sentence'].map(char_only)\n",
    "\n",
    "# Find the number of rows in the limiting class\n",
    "row_limit = min(data['author'].value_counts())\n",
    "\n",
    "# Balance classes\n",
    "drop_indices = []\n",
    "\n",
    "for author in data['author'].unique():\n",
    "    # Count of rows in current author class\n",
    "    class_count = len(data[data['author'] == author].values)\n",
    "    # Calculate rows to drop as the difference between current class and limiting class\n",
    "    drop_count = class_count - row_limit\n",
    "    drop_indices.extend(np.random.choice(data[data['author'] == author].index, \n",
    "                                         size=drop_count, \n",
    "                                         replace=False))\n",
    "\n",
    "# Drop the randomly chosen indices to balance classes\n",
    "data.drop(data.index[drop_indices], inplace=True)\n",
    "\n",
    "# Sanity check value vount\n",
    "data['author'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the late summer of that year we lived in a ...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the bed of the river there were pebbles and...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Troops went by the house and down the road and...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The trunks of the trees too were dusty and the...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The plain was rich with crops ; there were man...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>a farewell to arms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     author  \\\n",
       "0  In the late summer of that year we lived in a ...  hemingway   \n",
       "1  In the bed of the river there were pebbles and...  hemingway   \n",
       "2  Troops went by the house and down the road and...  hemingway   \n",
       "3  The trunks of the trees too were dusty and the...  hemingway   \n",
       "4  The plain was rich with crops ; there were man...  hemingway   \n",
       "\n",
       "                title  \n",
       "0  a farewell to arms  \n",
       "1  a farewell to arms  \n",
       "2  a farewell to arms  \n",
       "3  a farewell to arms  \n",
       "4  a farewell to arms  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78212"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word list for use in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words are stored with their authors in a tuple\n",
    "word_list = []\n",
    "for index, row in enumerate(data['sentence']):\n",
    "    \n",
    "    for word in row.split():\n",
    "        entry = (word, data['author'][index])\n",
    "    \n",
    "        word_list.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create author list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of authors to be used in EDA loops\n",
    "authors = ['hemingway', 'nabokov', 'vonnegut', 'woolf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmn_sentence_lengths = get_sentence_lengths('hemingway')\n",
    "nbk_sentence_lengths = get_sentence_lengths('nabokov')\n",
    "vng_sentence_lengths = get_sentence_lengths('vonnegut')\n",
    "wlf_sentence_lengths = get_sentence_lengths('woolf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max needed was 256 from virginia woolf\n",
    "X_sent = np.array(np.linspace(0, 256, 1000)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create KDE for sentence length distribution\n",
    "y_plot = get_kde_output(X_sent, wlf_sentence_lengths, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemingway white 143\n",
      "hemingway black 50\n",
      "hemingway red 57\n",
      "hemingway blue 54\n",
      "hemingway green 50\n",
      "hemingway yellow 44\n",
      "hemingway purple 21\n",
      "hemingway orange 3\n",
      "nabokov white 306\n",
      "nabokov black 460\n",
      "nabokov red 240\n",
      "nabokov blue 266\n",
      "nabokov green 191\n",
      "nabokov yellow 65\n",
      "nabokov purple 41\n",
      "nabokov orange 37\n",
      "vonnegut white 109\n",
      "vonnegut black 60\n",
      "vonnegut red 44\n",
      "vonnegut blue 78\n",
      "vonnegut green 45\n",
      "vonnegut yellow 45\n",
      "vonnegut purple 6\n",
      "vonnegut orange 19\n",
      "woolf white 283\n",
      "woolf black 129\n",
      "woolf red 193\n",
      "woolf blue 148\n",
      "woolf green 191\n",
      "woolf yellow 135\n",
      "woolf purple 56\n",
      "woolf orange 16\n"
     ]
    }
   ],
   "source": [
    "color_list = ['white', 'black', 'red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "\n",
    "for author in authors:\n",
    "    for color in color_list:\n",
    "        print(f'{author} {color} {word_count(word_list, author, color)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmn_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'hemingway':\n",
    "        hmn_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbk_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'nabokov':\n",
    "        nbk_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "vng_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'vonnegut':\n",
    "        vng_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlf_word_lengths = []\n",
    "\n",
    "for word in word_list:\n",
    "    if word[1] == 'woolf':\n",
    "        wlf_word_lengths.append(len(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded by 0 to the largest word at 18 chars\n",
    "X_word = np.array(np.linspace(0, 18, 1000)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create KDE for word length distribution\n",
    "y_plot = get_kde_output(X_word, wlf_word_lengths, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting counts of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemingway total nouns: 8326\n",
      "nabokov total nouns: 22413\n",
      "vonnegut total nouns: 16109\n",
      "woolf total nouns: 11812\n"
     ]
    }
   ],
   "source": [
    "for author in authors:\n",
    "    nouns = []\n",
    "    \n",
    "    for row in data[data['author'] == author]['sentence']:\n",
    "        for word in row.split():\n",
    "            if word == 'nnnn':\n",
    "                nouns.append(word)\n",
    "                \n",
    "    print(f'{author} total nouns: {len(nouns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026243046034072463"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide by authors word count for rate\n",
    "8326/317265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get totals for scaling charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemingway total sentences: 26661\n",
      "nabokov total sentences: 17504\n",
      "vonnegut total sentences: 18172\n",
      "woolf total sentences: 15560\n"
     ]
    }
   ],
   "source": [
    "for author in authors:\n",
    "    total_sents = []\n",
    "\n",
    "    for row in data[data['author'] == author]['sentence']:\n",
    "        total_sents.append(row)\n",
    "\n",
    "    print(f'{author} total sentences: {len(total_sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemingway total words: 317265\n",
      "nabokov total words: 506732\n",
      "vonnegut total words: 274263\n",
      "woolf total words: 332285\n"
     ]
    }
   ],
   "source": [
    "for author in authors:\n",
    "    total_words = []\n",
    "\n",
    "    for row in data[data['author'] == author]['sentence']:\n",
    "        for word in row.split():\n",
    "            total_words.append(word)\n",
    "\n",
    "    print(f'{author} total words: {len(total_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "Save the DataFrame to a .csv file to be read by the modeling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = './REPLACE.csv'\n",
    "\n",
    "# data.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
